{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT TO FILTER BERNIE SANDERS TWEETS FROM the 2016_OTHER collection**\n",
    "\n",
    "    FKA TEST SCRIPT TO EXTRACT DAILY TWEET VOLUME DATA FOR 2012 (2014, 2016, 2018) ELECTIONS WITH BASIC REPLICABLE KEYWORDS: DEMOCRAT/REPUBLICAN\n",
    "\n",
    "    NL, 16/05/19\n",
    "    NL, 23/05/19 -- getting towards deployment with help from Frido\n",
    "    NL, 04/06/19 -- new edits after local testing\n",
    "    ----\n",
    "    NL, 12/12/19 -- adapted for re-work of sentiment paper for NL PhD. There are different keywords and different target collections to take this from, but essentially it's the same principle. \n",
    "    NL, 13/01/20 -- trying to get this script to work for the bernie thing \n",
    "    NL, 14/01/20 -- first beta testing stage for bernie script --> going to standard python script\n",
    "    NL, 16/01/20 -- it worked! re-doing the same thing for the hillary collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from bz2 import BZ2File as bzopen\n",
    "import json\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glob out the relevant collection, which is: us_election_hillary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an array with all the files to be read in\n",
    "collection_files = glob.glob(\"/scratch/olympus/us_election_hillary_2016/data/*.bz2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out the time frame of interest. \n",
    "\n",
    "The time frame of interest is 3 weeks leading up to each of 3 primaries: **New Hampshire [February 9, 2016], South Carolina [February 27, 2016], Massachusetts [March 1, 2016]**. \n",
    "\n",
    "To keep it safe, I'm getting tweets from 01/01 to 02/03. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the files for the other collections too!! \n",
    "my_delta = datetime.timedelta(days = 1)\n",
    "\n",
    "# relevant date range: \n",
    "# create an array of all pertinent days\n",
    "start = datetime.datetime.strptime(\"2016-01-01\", \"%Y-%m-%d\")\n",
    "end = datetime.datetime.strptime(\"2016-03-02\", \"%Y-%m-%d\")\n",
    "date_generated = [start + datetime.timedelta(days=x) for x in range(0, (end-start).days)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dates strings turns the dates into the right format for extracting the right collection files.\n",
    "dates_strings = []\n",
    "\n",
    "for date in date_generated:\n",
    "    pos1 = date.strftime(\"%m_%d_%Y\")\n",
    "    dates_strings.append(pos1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pertinent_files = [x for x in collection_files if sum([int(z in x) for z in dates_strings]) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up regex of interest\n",
    "hillary = \"|\".join([\"hillary\", \"clinton\", \"rodham\"])\n",
    "hillary = re.compile(hillary, re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions for finding the regex and iterating through all relevant gzipped tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function that finds whether there's a mention of reps or dems in a string\n",
    "\n",
    "def contains_regex(tweet, regex):\n",
    "    pos1 = regex.search(tweet)\n",
    "    if type(pos1) is re.Match:\n",
    "        output = True\n",
    "    else:\n",
    "        output = False\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that goes through one gzipped tweet file, and checks every line for the target keywords, \n",
    "# and writes out if it's there. \n",
    "\n",
    "def process_file(path, output_path):\n",
    "    output_length = []\n",
    "    with bzopen(path) as infile: # unzips the file\n",
    "        filename = path\n",
    "        print(filename)\n",
    "        for i,line in enumerate(infile):\n",
    "            try:\n",
    "                tweet = json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f'Error in {infile} line {i}: {e}')\n",
    "            if not 'text' in tweet: # this jumps to the next line in the json if there's no text\n",
    "                    print(f'Error in {infile} line {i}: No text element to read from')\n",
    "                    continue\n",
    "            else:\n",
    "                text_field = tweet['text']\n",
    "            if contains_regex(tweet=text_field, regex=hillary):\n",
    "                #output.append(tweet)\n",
    "                with open(output_path, 'a+') as outfile:\n",
    "                    pos1 = json.dumps(tweet)\n",
    "                    outfile.write(pos1 + '\\n')\n",
    "    \n",
    "    print('Wrote tweets to json. Next file.')\n",
    "    \n",
    "    #return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, the loop that runs this thing for the entirety of the files of interest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, build a list with names for the output files\n",
    "\n",
    "output_filenames = []\n",
    "\n",
    "for i in range(len(pertinent_files)):\n",
    "    pos1 = \"/scratch/nl1676/olympus_local/sentiment_paper_relevant_tweets/hillary_tweets_{}.json\".format(dates_strings[i])\n",
    "    output_filenames.append(pos1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/olympus/us_election_hillary_2016/data/us_election_hillary_2016_data__02_08_2016__00_00_00__23_59_59.json.bz2\n",
      "Wrote tweets to json. Next file.\n"
     ]
    }
   ],
   "source": [
    "process_file(pertinent_files[0], output_filenames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so, now we have 2 lists with input arugments of equal length.\n",
    "# solution for parallel processing with multiple function arguments adapted\n",
    "# from http://www.erogol.com/passing-multiple-arguments-python-multiprocessing-pool/, accessed 03/06/19\n",
    "\n",
    "# auxiliary function to make it work\n",
    "def par_helper(args):\n",
    "   return process_file(*args)\n",
    "\n",
    "def parallel_product(arg_a, arg_b):\n",
    "   # spark given number of processes\n",
    "   pool = Pool(5)\n",
    "   # set each matching item into a tuple\n",
    "   job_args = [(item_a, arg_b[i]) for i, item_a in enumerate(arg_a)]\n",
    "   # map to pool\n",
    "   pool.map(par_helper, job_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_product(pertinent_files, output_filenames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
